{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "9ad05ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "a95f0aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runSnake(mode, num_obstacles):\n",
    "    command = ['python', 'snake_starter.py',  f'{mode}', f'{num_obstacles}']\n",
    "    p = subprocess.run(command, capture_output=True).stdout\n",
    "    try:\n",
    "        return int(p.decode()[17:])\n",
    "    except:\n",
    "        #I'm getting some occasional keyerrors from astarAI and I don't have the patience to debug.\n",
    "        return runSnake(mode, num_obstacles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "7cf38b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = ['astar', 'dijkstra', 'greedy']\n",
    "num_obstacles = 0\n",
    "trials = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c5b253d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'astar': 90.7, 'dijkstra': 82.8, 'greedy': 22.166666666666668}\n"
     ]
    }
   ],
   "source": [
    "avg_scores = {}\n",
    "for mode in modes:\n",
    "    avg_scores[mode] = 0\n",
    "    for _ in range(trials):\n",
    "        avg_scores[mode] += runSnake(mode, num_obstacles)\n",
    "    avg_scores[mode] = avg_scores[mode]/trials\n",
    "print(avg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "c8fb3c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_obstacles = 10\n",
    "modes = ['astar','dijkstra', 'greedy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "93dfa3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'astar': 88.46666666666667, 'dijkstra': 80.26666666666667, 'greedy': 4.633333333333334}\n"
     ]
    }
   ],
   "source": [
    "avg_scores = {}\n",
    "for mode in modes:\n",
    "    avg_scores[mode] = 0\n",
    "    for _ in range(trials):\n",
    "        avg_scores[mode] += runSnake(mode, num_obstacles)\n",
    "    avg_scores[mode] = avg_scores[mode]/trials\n",
    "print(avg_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a293b1",
   "metadata": {},
   "source": [
    "Greedy is obviously terrible because it doesn't cosider the position of obstacles or the snake's body. Astar on average performs 8 points better than dijkstra's both with and without obstacles, but (while I would like to believe astar performs better because I made it myself), I believe that the difference in performance between dijkstra's and astar is statistically insignificant. Assuming both algorithmns work properly, astar and dijstra's will always find the shortest path between the snake head and any given node. This means that for two identical boards, where the obstacles and apples always appear in the same position, dijkstra's and astar should find the exact same path to the next apple, meaning the scores should be identical. Perhaps for some apples, multiple shortest paths exist, which might explain the slight difference in performance. A few times when I ran the test, dijstra's performed better than astar, so it's likely that the difference in performance would approach zero as more trials are ran. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
